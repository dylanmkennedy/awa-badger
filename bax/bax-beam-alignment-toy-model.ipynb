{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90ac8fe9",
   "metadata": {},
   "source": [
    "# Centroid alignment using Xopt/BAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2666c7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b2005f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\Dylan\\\\SLAC') #parent directory containing emitopt module\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from xopt import Xopt\n",
    "from xopt.vocs import VOCS\n",
    "from xopt.generators.bayesian.bax_generator import BaxGenerator\n",
    "from xopt.generators.bayesian.expected_improvement import ExpectedImprovementGenerator\n",
    "\n",
    "from xopt.evaluator import Evaluator\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409ab078",
   "metadata": {},
   "source": [
    "# Pathwise sampling for Linear Product Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c9fa0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "\n",
    "def draw_poly_kernel_prior_paths(\n",
    "    poly_kernel, n_samples\n",
    "):  # poly_kernel is a scaled polynomial kernel\n",
    "    c = poly_kernel.offset\n",
    "    degree = poly_kernel.power\n",
    "    ws = torch.randn(size=[n_samples, 1, degree + 1], device=c.device)\n",
    "\n",
    "    def paths(xs):\n",
    "        if (\n",
    "            len(xs.shape) == 2 and xs.shape[1] == 1\n",
    "        ):  # xs must be n_samples x npoints x 1 dim\n",
    "            xs = xs.repeat(n_samples, 1, 1)  # duplicate over batch (sample) dim\n",
    "\n",
    "        coeffs = [math.comb(degree, i) for i in range(degree + 1)]\n",
    "        X = torch.concat(\n",
    "            [\n",
    "                (coeff * c.pow(i)).sqrt() * xs.pow(degree - i)\n",
    "                for i, coeff in enumerate(coeffs)\n",
    "            ],\n",
    "            dim=2,\n",
    "        )\n",
    "        W = ws.repeat(1, xs.shape[1], 1)  # ws is n_samples x 1 x 3 dim\n",
    "\n",
    "        phis = W * X\n",
    "        return torch.sum(phis, dim=-1)  # result tensor is shape n_samples x npoints\n",
    "\n",
    "    return paths\n",
    "\n",
    "def draw_linear_product_kernel_prior_paths(model, n_samples):\n",
    "    ndim = model.train_inputs[0].shape[1]\n",
    "\n",
    "    outputscale = copy.copy(model.covar_module.outputscale.detach())\n",
    "    kernels = []\n",
    "    dims = []\n",
    "\n",
    "    for i in range(len(model.covar_module.base_kernel.kernels)):\n",
    "        lin_kernel = copy.deepcopy(model.covar_module.base_kernel.kernels[i])\n",
    "        kernels += [lin_kernel]\n",
    "        dims += [lin_kernel.active_dims]\n",
    "\n",
    "    lin_prior_paths = [\n",
    "        draw_poly_kernel_prior_paths(kernel, n_samples) for kernel in kernels\n",
    "    ]\n",
    "\n",
    "    def linear_product_kernel_prior_paths(xs):\n",
    "        ys_lin = []\n",
    "        for i in range(len(lin_prior_paths)):\n",
    "            xs_lin = torch.index_select(xs, dim=-1, index=dims[i]).float()\n",
    "            ys_lin += [lin_prior_paths[i](xs_lin)]\n",
    "        output = 1.0\n",
    "        for ys in ys_lin:\n",
    "            output *= ys\n",
    "        return (outputscale.sqrt() * output).double()\n",
    "\n",
    "    return linear_product_kernel_prior_paths\n",
    "\n",
    "def draw_linear_product_kernel_post_paths(model, n_samples, cpu=True):\n",
    "    linear_product_kernel_prior_paths = draw_linear_product_kernel_prior_paths(\n",
    "        model, n_samples=n_samples\n",
    "    )\n",
    "\n",
    "    train_x = model.train_inputs[0]\n",
    "\n",
    "    train_y = model.train_targets.reshape(-1, 1)\n",
    "\n",
    "    train_y = train_y - model.mean_module(train_x).reshape(train_y.shape)\n",
    "\n",
    "    Knn = model.covar_module(train_x, train_x)\n",
    "\n",
    "    sigma = torch.sqrt(model.likelihood.noise[0])\n",
    "\n",
    "    K = Knn + sigma**2 * torch.eye(Knn.shape[0])\n",
    "\n",
    "    prior_residual = train_y.repeat(n_samples, 1, 1).reshape(\n",
    "        n_samples, -1\n",
    "    ) - linear_product_kernel_prior_paths(train_x)\n",
    "    prior_residual -= sigma * torch.randn_like(prior_residual)\n",
    "\n",
    "    Lnn = torch.cholesky(K.to_dense())\n",
    "    batched_lnn = torch.stack([Lnn] * n_samples)\n",
    "    batched_lnnt = torch.stack([Lnn.T] * n_samples)\n",
    "\n",
    "    vbar = torch.linalg.solve(batched_lnn, prior_residual)\n",
    "    v = torch.linalg.solve(batched_lnnt, vbar)\n",
    "    v = v.reshape(-1, 1)\n",
    "\n",
    "    v = v.reshape(n_samples, -1, 1)\n",
    "    v_t = v.transpose(1, 2)\n",
    "\n",
    "    def post_paths(xs):\n",
    "        if model.input_transform is not None:\n",
    "            xs = model.input_transform(xs)\n",
    "\n",
    "        K_update = model.covar_module(train_x, xs.double())\n",
    "\n",
    "        update = torch.matmul(v_t, K_update)\n",
    "        update = update.reshape(n_samples, -1)\n",
    "\n",
    "        prior = linear_product_kernel_prior_paths(xs)\n",
    "\n",
    "        post = prior + update + model.mean_module(xs)\n",
    "        if model.outcome_transform is not None:\n",
    "            post = model.outcome_transform.untransform(post)[0]\n",
    "\n",
    "        return post\n",
    "\n",
    "    post_paths.n_samples = n_samples\n",
    "\n",
    "    return post_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffae3f0",
   "metadata": {},
   "source": [
    "# Alignment Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4679d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xopt.generators.bayesian.bax.algorithms import Algorithm\n",
    "from abc import ABC\n",
    "from pydantic import Field\n",
    "from scipy.optimize import minimize\n",
    "from torch import Tensor\n",
    "from botorch.models.model import Model, ModelList\n",
    "from typing import Dict, Tuple, Union\n",
    "\n",
    "def unif_random_sample_domain(n_samples, domain):\n",
    "    ndim = len(domain)\n",
    "\n",
    "    # uniform sample, rescaled, and shifted to cover the domain\n",
    "    x_samples = torch.rand(n_samples, ndim) * torch.tensor(\n",
    "        [bounds[1] - bounds[0] for bounds in domain]\n",
    "    ) + torch.tensor([bounds[0] for bounds in domain])\n",
    "\n",
    "    return x_samples\n",
    "\n",
    "class ScipyBeamAlignment(Algorithm, ABC):\n",
    "    name = \"ScipyBeamAlignment\"\n",
    "    meas_dims: Union[int, list[int]] = Field(\n",
    "        description=\"dimension indeces of the quads through which the beam will be aligned\"\n",
    "    )\n",
    "    centroid_position_keys : list[str] = Field(\n",
    "        description=\"names of centroid position outputs used to measure alignment\")\n",
    "\n",
    "\n",
    "    @property\n",
    "    def model_names_ordered(self) -> list:  \n",
    "        # get observable model names in the order they appear in the model (ModelList)\n",
    "        return [name for name in self.centroid_position_keys]\n",
    "    \n",
    "    def get_execution_paths(\n",
    "        self, model: Model, bounds: Tensor\n",
    "    ) -> Tuple[Tensor, Tensor, Dict]:\n",
    "        \"\"\"get execution paths that minimize the objective function\"\"\"\n",
    "\n",
    "        x_stars_all, xs, ys, post_paths_cpu = self.get_sample_optimal_tuning_configs(\n",
    "            model.models[0], bounds, cpu=False\n",
    "        )\n",
    "\n",
    "        xs_exe = xs\n",
    "        ys_exe = ys.reshape(*ys.shape, 1)\n",
    "\n",
    "        results_dict = {\n",
    "            \"xs_exe\": xs_exe,\n",
    "            \"ys_exe\": ys_exe,\n",
    "            \"X_stars\": x_stars_all,\n",
    "            \"post_paths_cpu\": post_paths_cpu,\n",
    "        }\n",
    "\n",
    "        return xs_exe, ys_exe, results_dict\n",
    "\n",
    "    def get_sample_optimal_tuning_configs(\n",
    "        self, model: Model, bounds: Tensor, verbose=False, cpu=False\n",
    "    ):\n",
    "        meas_scans = torch.index_select(\n",
    "            bounds.T, dim=0, index=torch.tensor(self.meas_dims)\n",
    "        )\n",
    "        ndim = bounds.shape[1]\n",
    "        tuning_dims = [i for i in range(ndim) if i not in self.meas_dims]\n",
    "        tuning_domain = torch.index_select(\n",
    "            bounds.T, dim=0, index=torch.tensor(tuning_dims)\n",
    "        )\n",
    "\n",
    "        device = torch.tensor(1).device\n",
    "        torch.set_default_tensor_type(\"torch.DoubleTensor\")\n",
    "\n",
    "        cpu_model = copy.deepcopy(model).cpu()\n",
    "\n",
    "        post_paths_cpu = draw_linear_product_kernel_post_paths(\n",
    "            cpu_model, n_samples=self.n_samples\n",
    "        )\n",
    "\n",
    "        xs_tuning_init = unif_random_sample_domain(\n",
    "            self.n_samples, tuning_domain\n",
    "        ).double()\n",
    "\n",
    "        x_tuning_init = xs_tuning_init.flatten()\n",
    "\n",
    "        # minimize\n",
    "        def target_func_for_scipy(x_tuning_flat):\n",
    "            return (\n",
    "                self.sum_samplewise_misalignment_flat_x(\n",
    "                    post_paths_cpu,\n",
    "                    torch.tensor(x_tuning_flat),\n",
    "                    self.meas_dims,\n",
    "                    meas_scans.cpu(),\n",
    "                )\n",
    "                .detach()\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "\n",
    "        def target_func_for_torch(x_tuning_flat):\n",
    "            return self.sum_samplewise_misalignment_flat_x(\n",
    "                post_paths_cpu, x_tuning_flat, self.meas_dims, meas_scans.cpu()\n",
    "            )\n",
    "\n",
    "        def target_jac(x):\n",
    "            return (\n",
    "                torch.autograd.functional.jacobian(\n",
    "                    target_func_for_torch, torch.tensor(x)\n",
    "                )\n",
    "                .detach()\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "\n",
    "        res = minimize(\n",
    "            target_func_for_scipy,\n",
    "            x_tuning_init.detach().cpu().numpy(),\n",
    "            jac=target_jac,\n",
    "            bounds=tuning_domain.repeat(self.n_samples, 1).detach().cpu().numpy(),\n",
    "            options={\"eps\": 1e-03},\n",
    "        )\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"ScipyBeamAlignment evaluated\",\n",
    "                self.n_samples,\n",
    "                \"(pathwise) posterior samples\",\n",
    "                res.nfev,\n",
    "                \"times in get_sample_optimal_tuning_configs().\",\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                \"ScipyBeamAlignment evaluated\",\n",
    "                self.n_samples,\n",
    "                \"(pathwise) posterior sample jacobians\",\n",
    "                res.njev,\n",
    "                \"times in get_sample_optimal_tuning_configs().\",\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                \"ScipyBeamAlignment took\",\n",
    "                res.nit,\n",
    "                \"steps in get_sample_optimal_tuning_configs().\",\n",
    "            )\n",
    "\n",
    "        x_stars_flat = torch.tensor(res.x)\n",
    "\n",
    "        x_stars_all = x_stars_flat.reshape(\n",
    "            self.n_samples, -1\n",
    "        )  # each row represents its respective sample's optimal tuning config\n",
    "\n",
    "        misalignment, xs, ys = self.post_path_misalignment(\n",
    "            post_paths_cpu,\n",
    "            x_stars_all,  # n x d tensor\n",
    "            self.meas_dims,  # list of integers\n",
    "            meas_scans.cpu(),  # tensor of measurement device(s) scan inputs, shape: len(meas_dims) x 2\n",
    "            samplewise=True,\n",
    "        )\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            torch.set_default_tensor_type(\"torch.cuda.DoubleTensor\")\n",
    "\n",
    "        if cpu:\n",
    "            return x_stars_all, xs, ys, post_paths_cpu  # x_stars should still be on cpu\n",
    "        else:\n",
    "            return x_stars_all.to(device), xs.to(device), ys.to(device), post_paths_cpu\n",
    "        \n",
    "    def post_path_misalignment(\n",
    "        self,\n",
    "        post_paths,\n",
    "        x_tuning,  # n x d tensor\n",
    "        meas_dims,  # list of integers\n",
    "        meas_scans,  # tensor of measurement device(s) scan inputs, shape: len(meas_dims) x 2\n",
    "        samplewise=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A function that computes the beam misalignment(s) through a set of measurement quadrupoles\n",
    "        from a set of pathwise samples taken from a SingleTaskGP model of the beam centroid position with\n",
    "        respect to some tuning devices and some measurement quadrupoles.\n",
    "\n",
    "        arguments:\n",
    "            post_paths: a pathwise posterior sample from a SingleTaskGP model of the beam centroid\n",
    "                        position (assumes Linear ProductKernel)\n",
    "            x_tuning: a tensor of shape (n_samples x n_tuning_dims) where the nth row defines a point in\n",
    "                        tuning-parameter space at which to evaluate the misalignment of the nth\n",
    "                        posterior pathwise sample given by post_paths\n",
    "            meas_dims: the dimension indeces of our model that describe the quadrupole measurement devices\n",
    "            meas_scans: a tensor of measurement scan inputs, shape len(meas_dims) x 2, where the nth row\n",
    "                        contains two input scan values for the nth measurement quadrupole\n",
    "            samplewise: boolean. Set to False if you want to evaluate the misalignment for every point on\n",
    "                        every sample. If set to True, the misalignment for the nth sample (given by post_paths)\n",
    "                        will only be evaluated at the nth point (given by x_tuning). If samplewise is set to\n",
    "                        True, x_tuning must be shape n_samples x n_tuning_dims\n",
    "\n",
    "         returns:\n",
    "             misalignment: the sum of the squared slopes of the beam centroid model output with respect to the\n",
    "                             measurement quads\n",
    "             xs: the virtual scan inputs\n",
    "             ys: the virtual scan outputs (beam centroid positions)\n",
    "\n",
    "        NOTE: meas scans only needs to have 2 values for each device because it is expected that post_paths\n",
    "                are produced from a SingleTaskGP with Linear ProductKernel (i.e. post_paths should have\n",
    "                linear output for each dimension).\n",
    "        \"\"\"\n",
    "        n_steps_meas_scan = 1 + len(meas_dims)\n",
    "        n_tuning_configs = x_tuning.shape[0]\n",
    "\n",
    "        # construct measurement scan inputs\n",
    "        xs = torch.repeat_interleave(x_tuning, n_steps_meas_scan, dim=0)\n",
    "\n",
    "        for i in range(len(meas_dims)):\n",
    "            meas_dim = meas_dims[i]\n",
    "            meas_scan = meas_scans[i]\n",
    "            full_scan_column = meas_scan[0].repeat(n_steps_meas_scan, 1)\n",
    "            full_scan_column[i + 1, 0] = meas_scan[1]\n",
    "            full_scan_column_repeated = full_scan_column.repeat(n_tuning_configs, 1)\n",
    "\n",
    "            xs = torch.cat(\n",
    "                (xs[:, :meas_dim], full_scan_column_repeated, xs[:, meas_dim:]), dim=1\n",
    "            )\n",
    "\n",
    "        if samplewise:\n",
    "            xs = xs.reshape(n_tuning_configs, n_steps_meas_scan, -1)\n",
    "\n",
    "        ys = post_paths(xs)\n",
    "        ys = ys.reshape(-1, n_steps_meas_scan)\n",
    "\n",
    "        rise = ys[:, 1:] - ys[:, 0].reshape(-1, 1)\n",
    "        run = (meas_scans[:, 1] - meas_scans[:, 0]).T.repeat(ys.shape[0], 1)\n",
    "        slope = rise / run\n",
    "\n",
    "        misalignment = slope.pow(2).sum(dim=1)\n",
    "\n",
    "        if not samplewise:\n",
    "            ys = ys.reshape(-1, n_tuning_configs, n_steps_meas_scan)\n",
    "            misalignment = misalignment.reshape(-1, n_tuning_configs)\n",
    "\n",
    "        return misalignment, xs, ys\n",
    "    \n",
    "    def sum_samplewise_misalignment_flat_x(\n",
    "        self, post_paths, x_tuning_flat, meas_dims, meas_scans\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A wrapper function that computes the sum of the samplewise misalignments for more convenient\n",
    "        minimization with scipy.\n",
    "\n",
    "        arguments:\n",
    "            Same as post_path_misalignment() EXCEPT:\n",
    "\n",
    "            x_tuning_flat: a FLATTENED tensor formerly of shape (n_samples x ndim) where the nth\n",
    "                            row defines a point in tuning-parameter space at which to evaluate the\n",
    "                            misalignment of the nth posterior pathwise sample given by post_paths\n",
    "\n",
    "            NOTE: x_tuning_flat must be 1d (flattened) so the output of this function can be minimized\n",
    "                    with scipy minimization routines (that expect a 1d vector of inputs)\n",
    "            NOTE: samplewise is set to True to avoid unncessary computation during simultaneous minimization\n",
    "                    of the pathwise misalignments.\n",
    "        \"\"\"\n",
    "\n",
    "        x_tuning = x_tuning_flat.double().reshape(post_paths.n_samples, -1)\n",
    "\n",
    "        return torch.sum(\n",
    "            self.post_path_misalignment(\n",
    "                post_paths, x_tuning, meas_dims, meas_scans, samplewise=True\n",
    "            )[0]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1f6014",
   "metadata": {},
   "source": [
    "# Notebook settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9323010",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = False #whether to add noise to the ground-truth beam size function outputs\n",
    "n_obs_init = 3 #number of random initial observations for GP model\n",
    "n_samples = 100 #number of posterior samples for BAX\n",
    "rand_seed = 0\n",
    "\n",
    "#random seeds for reproducibility \n",
    "torch.manual_seed(rand_seed)\n",
    "np.random.seed(rand_seed) #only affects initial random observations through Xopt\n",
    "random.seed(rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87785524",
   "metadata": {},
   "source": [
    "# Build test function from single-quadrupole optical beam size model \n",
    "Here we define a simple ground-truth beam size function for our optimization problem, where we attempt to find the location in tuning parameter space with minimal emittance. Note that the function \"test_func\" used to evaluate the ground-truth beam size function takes a dictionary as input and returns a dictionary as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b73bcfe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define test functions\n",
    "var_names = ['x0', 'x1']\n",
    "\n",
    "def centroid_position_at_screen(x):\n",
    "    r0 = 0.0\n",
    "    cpas = (r0 + x[:,0]) + (r0 + x[:,0])*x[:,1]\n",
    "    \n",
    "#     return cpas * (1. + .1*torch.randn_like(cpas))\n",
    "    return cpas \n",
    "\n",
    "def test_func(input_dict):\n",
    "    x0 = torch.tensor(input_dict['x0']).reshape(-1,1)\n",
    "    x1 = torch.tensor(input_dict['x1']).reshape(-1,1)\n",
    "    x = torch.cat((x0, x1), dim=1)\n",
    "    return {'y': float(centroid_position_at_screen(x).squeeze().cpu().numpy())}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d2c09a",
   "metadata": {},
   "source": [
    "# Construct vocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dfbf20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable_names = ['x0', 'x1']\n",
      "domain =\n",
      " [[-2.  2.]\n",
      " [-2.  2.]]\n"
     ]
    }
   ],
   "source": [
    "variables = {var_name: [-2,2] for var_name in var_names}\n",
    "\n",
    "#construct vocs\n",
    "vocs = VOCS(\n",
    "    variables = variables,\n",
    "    observables = ['y']\n",
    ")\n",
    "\n",
    "print('variable_names =', vocs.variable_names)\n",
    "print('domain =\\n', vocs.bounds.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4651f47",
   "metadata": {},
   "source": [
    "# Prepare generator.\n",
    "In this example, we use a specialty covariance module (Linear ProductKernel) for our beam size model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02894212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.kernels import MaternKernel, PolynomialKernel, ScaleKernel\n",
    "from xopt.generators.bayesian.models.standard import StandardModelConstructor\n",
    "from gpytorch.priors.torch_priors import GammaPrior\n",
    "from xopt.generators.bayesian.bax_generator import BaxGenerator\n",
    "\n",
    "# from emitopt.algorithms import ScipyBeamAlignment\n",
    "\n",
    "# prepare custom covariance module\n",
    "# covar_module = PolynomialKernel(power=1)\n",
    "covar_module = PolynomialKernel(power=1, active_dims=[0]) * PolynomialKernel(power=1, active_dims=[1])\n",
    "scaled_covar_module = ScaleKernel(covar_module)   \n",
    "    \n",
    "# prepare options for Xopt generator\n",
    "covar_module_dict = {'y': scaled_covar_module}\n",
    "# covar_module_dict = {}\n",
    "model_constructor = StandardModelConstructor(covar_modules=covar_module_dict, use_low_noise_prior=True)\n",
    "\n",
    "algo = ScipyBeamAlignment(meas_dims=[1], n_samples=n_samples, centroid_position_keys=['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd586df8",
   "metadata": {},
   "source": [
    "# Construct generator, evaluator, Xopt objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7623bc33",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for BaxGenerator\nvocs\n  Value error, this generator only supports single objective [type=value_error, input_value=VOCS(variables={'x0': [-2...s={}, observables=['y']), input_type=VOCS]\n    For further information visit https://errors.pydantic.dev/2.3/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#construct BAX generator\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mBaxGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgp_constructor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_constructor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# generator = ExpectedImprovementGenerator(vocs, generator_options)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#construct evaluator\u001b[39;00m\n\u001b[0;32m      6\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m Evaluator(function\u001b[38;5;241m=\u001b[39mtest_func)\n",
      "File \u001b[1;32mc:\\users\\dylan\\slac\\xopt\\xopt\\generator.py:63\u001b[0m, in \u001b[0;36mGenerator.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    Initialize the generator.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     64\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialized generator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pydantic\\main.py:165\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    164\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m \u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__pydantic_self__\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for BaxGenerator\nvocs\n  Value error, this generator only supports single objective [type=value_error, input_value=VOCS(variables={'x0': [-2...s={}, observables=['y']), input_type=VOCS]\n    For further information visit https://errors.pydantic.dev/2.3/v/value_error"
     ]
    }
   ],
   "source": [
    "#construct BAX generator\n",
    "generator = BaxGenerator(vocs=vocs, gp_constructor=model_constructor, algorithm=algo)\n",
    "# generator = ExpectedImprovementGenerator(vocs, generator_options)\n",
    "\n",
    "#construct evaluator\n",
    "evaluator = Evaluator(function=test_func)\n",
    "\n",
    "#construct Xopt optimizer\n",
    "optimizer = Xopt(evaluator=evaluator, generator=generator, vocs=vocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed80d015",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.random_evaluate(n_obs_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ab907",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cd2302",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_centroid_model = optimizer.generator.train_model().models[0]\n",
    "\n",
    "ls = torch.linspace(-2,2,21)\n",
    "x0, x1 = torch.meshgrid(ls, ls)\n",
    "xmesh_serialized = torch.cat((x0.reshape(-1,1), x1.reshape(-1,1)), dim=1)\n",
    "ymesh_model = beam_centroid_model.posterior(xmesh_serialized).mean.reshape(21,21).detach()\n",
    "ymesh_model_var = beam_centroid_model.posterior(xmesh_serialized).variance.reshape(21,21).detach()\n",
    "ymesh_gt = centroid_position_at_screen(xmesh_serialized).reshape(21,21).detach()\n",
    "\n",
    "fig, axs = plt.subplots(1,3)\n",
    "fig.set_size_inches(9,5)\n",
    "\n",
    "ax = axs[0]\n",
    "c = ax.pcolormesh(x0, x1, ymesh_gt, vmin=-5, vmax=5)\n",
    "fig.colorbar(c)\n",
    "ax.set_xlabel('trim')\n",
    "ax.set_ylabel('quad')\n",
    "ax.set_title('Ground Truth Centroid Position')\n",
    "\n",
    "ax = axs[1]\n",
    "ax.pcolormesh(x0, x1, ymesh_model, vmin=-5, vmax=5)\n",
    "ax.set_xlabel('trim')\n",
    "ax.set_ylabel('quad')\n",
    "ax.set_title('Posterior Mean Centroid Position')\n",
    "ax.scatter(optimizer.data['x0'], optimizer.data['x1'], c=optimizer.data['y'],\n",
    "           vmin=-5, vmax=5, edgecolors='w', label='Observations')\n",
    "ax.legend()\n",
    "\n",
    "ax = axs[2]\n",
    "c = ax.pcolormesh(x0, x1, ymesh_model_var.sqrt())\n",
    "fig.colorbar(c)\n",
    "\n",
    "# for x_star in X_stars:\n",
    "#     ax.axvline(float(x_star), ymax=0.1, c='r')\n",
    "    \n",
    "ax.scatter(optimizer.data['x0'][:-1], optimizer.data['x1'][:-1], c='m', label='Observations')\n",
    "ax.scatter(optimizer.data['x0'][-1:], optimizer.data['x1'][-1:], c='r', marker='x', s=120)\n",
    "ax.set_title('Posterior Std Dev')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf64721",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('\\n\\nIteration', i, '\\n')\n",
    "    optimizer.step()\n",
    "#     X_stars = optimizer.generator.algo_results['X_stars']\n",
    "\n",
    "    beam_centroid_model = optimizer.generator.model.models[0]\n",
    "\n",
    "    ls = torch.linspace(-2,2,21)\n",
    "    x0, x1 = torch.meshgrid(ls, ls)\n",
    "    xmesh_serialized = torch.cat((x0.reshape(-1,1), x1.reshape(-1,1)), dim=1)\n",
    "    ymesh_model = beam_centroid_model.posterior(xmesh_serialized).mean.reshape(21,21).detach()\n",
    "    ymesh_model_var = beam_centroid_model.posterior(xmesh_serialized).variance.reshape(21,21).detach()\n",
    "    ymesh_gt = centroid_position_at_screen(xmesh_serialized).reshape(21,21).detach()\n",
    "\n",
    "    fig, axs = plt.subplots(1,3)\n",
    "    fig.set_size_inches(9,5)\n",
    "\n",
    "    ax = axs[0]\n",
    "    c = ax.pcolormesh(x0, x1, ymesh_gt, vmin=-5, vmax=5)\n",
    "    fig.colorbar(c)\n",
    "    ax.set_xlabel('trim')\n",
    "    ax.set_ylabel('quad')\n",
    "    ax.set_title('Ground Truth Centroid Position')\n",
    "\n",
    "    ax = axs[1]\n",
    "    ax.pcolormesh(x0, x1, ymesh_model, vmin=-5, vmax=5)\n",
    "    ax.set_xlabel('trim')\n",
    "    ax.set_ylabel('quad')\n",
    "    ax.set_title('Posterior Mean Centroid Position')\n",
    "    ax.scatter(optimizer.data['x0'][:-1], optimizer.data['x1'][:-1], c=optimizer.data['y'][:-1], \n",
    "               vmin=-5, vmax=5, edgecolors='w', label='Observations')\n",
    "    ax.scatter(optimizer.data['x0'][-1:], optimizer.data['x1'][-1:], c='r', \n",
    "               marker='x', s=120, label='Next Obs')\n",
    "\n",
    "    X_stars = optimizer.generator.algorithm_results['X_stars']\n",
    "    \n",
    "    ax.axvline(X_stars.flatten()[0], c='orange', ymax=0.05, label='Sample Optima')\n",
    "    for x_star in X_stars.flatten()[1:]:\n",
    "        ax.axvline(x_star, c='orange', ymax=0.05)\n",
    "    \n",
    "    ax.legend()\n",
    "    \n",
    "    ax = axs[2]\n",
    "    c = ax.pcolormesh(x0, x1, ymesh_model_var.sqrt())\n",
    "    fig.colorbar(c)\n",
    "\n",
    "    # for x_star in X_stars:\n",
    "    #     ax.axvline(float(x_star), ymax=0.1, c='r')\n",
    "\n",
    "    ax.scatter(optimizer.data['x0'][:-1], optimizer.data['x1'][:-1], c='m', label='Observations')\n",
    "    ax.scatter(optimizer.data['x0'][-1:], optimizer.data['x1'][-1:], c='r', marker='x', s=120)\n",
    "    ax.set_title('Posterior Std Dev')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.hist(X_stars.flatten(), density=True)\n",
    "    plt.title('Sampled Optimal Trim Settings')\n",
    "    plt.xlabel('Trim Setting')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11739e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e40246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
